{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e7844b-814f-4391-bb91-491d4673e612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b764268b-b5bb-4a18-b5a3-6b981e47b77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b13b4d2-35f8-4349-802a-9c27d8b6d48e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f5cb72-08cc-4507-8465-8e19ec1a6250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "length = 100\n",
    "n0 = 10\n",
    "paris_episodes = np.empty((0, length))\n",
    "for c in np.arange(0.01, .1, .02):\n",
    "    for m in np.arange(0.01, 4, .2):\n",
    "        paris = ParisLawDegradation(length=length, dim=1, c=c, m=m)\n",
    "        episodes_i = paris.generate_episode(x0=np.abs(np.random.randn(n0)*0.3+0.7))  # Initial crack lengths in meters\n",
    "        paris_episodes = np.concatenate([paris_episodes, episodes_i], axis=0)\n",
    "\n",
    "#drop invalid values\n",
    "paris_episodes = paris_episodes[~np.isnan(paris_episodes).any(axis=1)]\n",
    "paris_episodes = paris_episodes[(paris_episodes<15).all(axis=1)]\n",
    "paris_episodes = paris_episodes[(paris_episodes>=0).all(axis=1)]\n",
    "\n",
    "paris_episodes.shape\n",
    "# plot 20 random episodes\n",
    "plt.plot(paris_episodes[np.random.randint(0, paris_episodes.shape[0], size=100)].T)\n",
    "paris_episodes.min(), paris_episodes.max(), paris_episodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b1e7d1-3f02-43f2-8aa2-1e71ae100830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lin_episodes = np.empty((0, length))\n",
    "for c in np.linspace(0.01, 0.1, 50):\n",
    "    \n",
    "    lin = LinearDegradation(length=length, dim=1, c=c, mu_e=0, sigma_e=c/2)\n",
    "    episodes_i = lin.generate_episode(x0=np.abs(np.random.randn(n0)*0.3+0.7))  # Initial crack lengths in meters\n",
    "    lin_episodes = np.concatenate([lin_episodes, episodes_i], axis=0)\n",
    "\n",
    "#drop invalid values\n",
    "lin_episodes = lin_episodes[~np.isnan(lin_episodes).any(axis=1)]\n",
    "lin_episodes = lin_episodes[(lin_episodes<15).all(axis=1)]\n",
    "lin_episodes = lin_episodes[(lin_episodes>=0).all(axis=1)]\n",
    "\n",
    "# plot 20 random episodes\n",
    "plt.plot(lin_episodes[np.random.randint(0, lin_episodes.shape[0], size=100)].T)\n",
    "lin_episodes.min(), lin_episodes.max(), lin_episodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80cb3278-9437-4e1e-847e-4af245b64dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.linspace(2, 7, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73bc6090-68cb-4062-8197-d42fdf1c1974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shock_episodes = np.empty((0, length))\n",
    "for mu_t in range(2, 7):\n",
    "    for mu_shock in np.linspace(0.1, 0.3, 5):\n",
    "        shock = RandomShockDegradation(length=length, dim=1, \n",
    "                                       mu_t=mu_t, sigma_t=mu_t/3, \n",
    "                                       mu_shock=mu_shock, sigma_shock=mu_shock/3, \n",
    "                                       baseline=mu_shock/10)\n",
    "        \n",
    "        episodes_i = shock.generate_episode(x0=np.abs(np.random.randn(n0)*0.3+0.7))  # Initial crack lengths in meters\n",
    "        shock_episodes = np.concatenate([shock_episodes, episodes_i], axis=0)\n",
    "\n",
    "#drop invalid values\n",
    "shock_episodes = shock_episodes[~np.isnan(shock_episodes).any(axis=1)]\n",
    "shock_episodes = shock_episodes[(shock_episodes<15).all(axis=1)]\n",
    "shock_episodes = shock_episodes[(shock_episodes>=0).all(axis=1)]\n",
    "\n",
    "shock_episodes.shape\n",
    "# plot 20 random episodes\n",
    "plt.plot(shock_episodes[np.random.randint(0, shock_episodes.shape[0], size=100)].T)\n",
    "shock_episodes.min(), shock_episodes.max(), shock_episodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9325945c-ddc6-4d10-b219-3231ff0ccc13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sei_episodes = np.empty((0, length))\n",
    "for k in np.linspace(0.01, 2, 100):\n",
    "    \n",
    "    sei = SEILayer(length=length, dim=1, k=k, sigma_e=1)\n",
    "    \n",
    "    episodes_i = sei.generate_episode(x0=np.abs(np.random.randn(n0)*0.3+0.7))  # Initial crack lengths in meters\n",
    "    sei_episodes = np.concatenate([sei_episodes, episodes_i], axis=0)\n",
    "\n",
    "#drop invalid values\n",
    "sei_episodes = sei_episodes[~np.isnan(sei_episodes).any(axis=1)]\n",
    "sei_episodes = sei_episodes[(sei_episodes<15).all(axis=1)]\n",
    "sei_episodes = sei_episodes[(sei_episodes>=0).all(axis=1)]\n",
    "\n",
    "sei_episodes.shape\n",
    "# plot 20 random episodes\n",
    "plt.plot(sei_episodes[np.random.randint(0, sei_episodes.shape[0], size=200)].T)\n",
    "sei_episodes.min(), sei_episodes.max(), sei_episodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "952a7743-79bc-4490-a605-1337c7e25429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistic_episodes = np.empty((0, length))\n",
    "for alfa in np.linspace(0.01, .4, 40):\n",
    "    \n",
    "    logstiff = LogisticStiffness(length=length, dim=1, alfa=alfa, xmax=15, sigma_e=alfa)\n",
    "    \n",
    "    episodes_i = logstiff.generate_episode(x0=np.abs(np.random.randn(n0)*0.3+0.7))  # Initial crack lengths in meters\n",
    "    logistic_episodes = np.concatenate([logistic_episodes, episodes_i], axis=0)\n",
    "\n",
    "#drop invalid values\n",
    "logistic_episodes = logistic_episodes[~np.isnan(logistic_episodes).any(axis=1)]\n",
    "logistic_episodes = logistic_episodes[(logistic_episodes<15).all(axis=1)]\n",
    "logistic_episodes = logistic_episodes[(logistic_episodes>=0).all(axis=1)]\n",
    "\n",
    "logistic_episodes.shape\n",
    "# plot 20 random episodes\n",
    "plt.plot(logistic_episodes[np.random.randint(0, logistic_episodes.shape[0], size=200)].T)\n",
    "logistic_episodes.min(), logistic_episodes.max(), logistic_episodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a9788d-f382-48f2-8d61-f29751e5505f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "loglogistic_episodes = np.empty((0, length))\n",
    "for beta in np.linspace(0.01, .3, 8):\n",
    "    for k in np.linspace(-0.9, .9, 8):\n",
    "    \n",
    "        logstiff = LogLogisticStiffness(length=length, dim=1, alfa=15, beta=beta, k=k, sigma_e=beta)\n",
    "        \n",
    "        episodes_i = logstiff.generate_episode(x0=np.abs(np.random.randn(n0)*0.3+0.7))  # Initial crack lengths in meters\n",
    "        loglogistic_episodes = np.concatenate([loglogistic_episodes, episodes_i], axis=0)\n",
    "\n",
    "#drop invalid values\n",
    "loglogistic_episodes = loglogistic_episodes[~np.isnan(loglogistic_episodes).any(axis=1)]\n",
    "loglogistic_episodes = loglogistic_episodes[(loglogistic_episodes<15).all(axis=1)]\n",
    "loglogistic_episodes = loglogistic_episodes[(loglogistic_episodes>=0).all(axis=1)]\n",
    "\n",
    "loglogistic_episodes.shape\n",
    "# plot 20 random episodes\n",
    "plt.plot(loglogistic_episodes[np.random.randint(0, loglogistic_episodes.shape[0], size=200)].T)\n",
    "loglogistic_episodes.min(), loglogistic_episodes.max(), loglogistic_episodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4017daa4-8df4-41ea-a1b1-bf10bf9a1a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "episodes = np.concatenate([lin_episodes, shock_episodes, paris_episodes, sei_episodes, logistic_episodes, loglogistic_episodes], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5272ae9e-dd7b-4507-a963-0d89d4f45399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check default device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "#device = torch.device('cpu')  # Force CPU for consistency in this example\n",
    "print(\"Default Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9991925-4f22-46a1-90c7-dc39fa9d3d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Min: {episodes.min()}\")\n",
    "print(f\"Max: {episodes.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7c6fda-6854-4cd9-ab0b-972b17734c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Set up model and training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2211dd6f-8604-4e0d-af99-ef12864a659c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vocab_size=500\n",
    "context_window=40\n",
    "bs = 32\n",
    "# Split episodes into train/test (e.g., 80/20)\n",
    "# Shuffle indices\n",
    "shuffled_indices = np.random.permutation(len(episodes))\n",
    "\n",
    "# Apply shuffled indices\n",
    "shuffled_episodes = episodes[shuffled_indices]\n",
    "\n",
    "n_train = int(0.8 * len(episodes))\n",
    "train_episodes = shuffled_episodes[:n_train]\n",
    "test_episodes = shuffled_episodes[n_train:]\n",
    "train_dataset = TimeSeriesDataset(train_episodes, context_window=context_window, vocab_size=vocab_size)\n",
    "test_dataset = TimeSeriesDataset(test_episodes, context_window=context_window, vocab_size=vocab_size)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3263e1-658b-4ac6-9db2-dc6ae0b4726b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_episodes.shape, test_episodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68da8470-96a0-4e2a-9cee-2d2e168ed239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False, pin_memory=True, num_workers=2)\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2500a9d9-7fa8-4ab8-b597-792217cff674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = DegradationTransformer(vocab_size=vocab_size, context_window=context_window, \n",
    "                               embedding_dim=64, num_heads=8, num_blocks=3)\n",
    "# model param count\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b9260d-f1bc-41ac-9f98-dfbe1f0aebca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get lr form learner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbb51521-3cea-4da9-bd88-3877f1788d18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "len(train_loader), len(test_loader)\n",
    "cbs = [ProgressCallback(50), SaveModel(), MLflowCallback()]\n",
    "learner = Learner(model, optim, loss_func, train_loader, test_loader, cbs, device=device)\n",
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "877cfd91-f024-47ea-b8f7-c69ca39d8ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47049705-4cfd-46c4-b526-187c657cc3e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inexces = np.random.randint(0, test_episodes.shape[0], size=3)\n",
    "plt.plot(learner.predict(test_episodes[inexces,:context_window], num_periods=100-context_window).T, '.')\n",
    "plt.plot(test_episodes[inexces, ].T)\n",
    "\n",
    "np.save('degradation_episodes.npy', test_episodes[inexces,:context_window])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3a70c4-39cd-413a-8eee-1ae5a0668686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## next token prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a987ba7-c323-4983-be13-9fbfd0494c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDatasetRaw(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, context_window, vocab_size):\n",
    "        self.data = data\n",
    "        self.context_window = context_window\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_episodes, self.episode_length = data.shape\n",
    "        self.samples_per_episode = self.episode_length - context_window\n",
    "        self.normalizer = WindowNormalizer()\n",
    "        self.digitizer = UniformDigitizer(vocab_size)\n",
    "    def __len__(self):\n",
    "        return self.n_episodes * self.samples_per_episode\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "\n",
    "        idx_array = np.atleast_1d(idx)\n",
    "        single_sample = np.isscalar(idx)\n",
    "\n",
    "        episode_idx = idx_array // self.samples_per_episode\n",
    "        pos = idx_array % self.samples_per_episode\n",
    "\n",
    "        # Correct slicing for variable positions\n",
    "        x = np.stack([self.data[e, p:p+self.context_window] for e, p in zip(episode_idx, pos)])\n",
    "        y = np.array([self.data[e, p+self.context_window] for e, p in zip(episode_idx, pos)])\n",
    "\n",
    "        # Convert to tensors\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float).squeeze()\n",
    "\n",
    "        if single_sample:\n",
    "            x_tensor = x_tensor[0]\n",
    "            y_tensor = y_tensor[0]\n",
    "\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c68806-88c2-4739-879a-ec1c59cc0561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_episodes.shape, test_episodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc6a7ad-a7d5-48df-a052-49c0845edd45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d5fbc7f-8986-4c94-aac7-e91bd8544345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_raw= TimeSeriesDatasetRaw(train_episodes, context_window=context_window, vocab_size=vocab_size)\n",
    "x_batch, y_batch = dataset_raw[range(5_000)]\n",
    "\n",
    "\n",
    "print(x_batch.shape)\n",
    "\n",
    "y_predict = learner.predict(x_batch, num_periods=1)[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df=pd.DataFrame([y_batch, y_predict]).T\n",
    "df.columns = ['True Value', 'Predicted Value']\n",
    "fig = px.line(df, labels={ 'x':['Predicted Value','Predicted Value']}, title='Train data - Next Token Prediction Accuracy')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e2c4f4f-bba9-4414-8c99-891d74e327f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "does the plot above means we need a bit of regularization, it seems we are fitting to noise??\n",
    "\n",
    "we are also overfitting to the near-zero scienarios, most of which are paris episodes??!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c134b2-c826-4efa-b7d5-6ce2640eedee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(y_batch, y_predict, '.', alpha=0.2)\n",
    "plt.xlabel('True Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Train data - Next Token Prediction Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36b3f27-d1c2-49b0-8b55-17d2bd0f72a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_raw= TimeSeriesDatasetRaw(test_episodes, context_window=context_window, vocab_size=vocab_size)\n",
    "x_batch, y_batch = dataset_raw[range(5_000)]\n",
    "\n",
    "\n",
    "print(x_batch.shape)\n",
    "\n",
    "y_predict = learner.predict(x_batch, num_periods=1)[:, -1]\n",
    "\n",
    "\n",
    "df=pd.DataFrame([y_batch, y_predict]).T\n",
    "df.columns = ['True Value', 'Predicted Value']\n",
    "fig = px.line(df, labels={ 'x':['Predicted Value','Predicted Value']}, title='Test data - Next Token Prediction Accuracy')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afaf3042-d903-447f-ad6c-88df31cac36a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(y_batch, y_predict, '.', alpha=0.2)\n",
    "plt.xlabel('True Value')\n",
    "plt.ylabel('Predicted Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f9bb7b-b088-4d67-9e8f-ba861ab882d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# test model loading from local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7db4ee-a2b1-44a8-98f9-0c6936f4aa41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from safetensors.torch import load_model\n",
    "import json\n",
    "with open(\"degradation_transformer_model_config.json\", \"rb\") as f:\n",
    "    model_params=json.load(f)\n",
    "\n",
    "model1 = DegradationTransformer(vocab_size=model_params['vocab_size'], \n",
    "                                context_window=model_params['context_window'], \n",
    "                               embedding_dim=model_params['embedding_dim'], \n",
    "                               num_heads=model_params['num_heads'],\n",
    "                                 num_blocks=model_params['num_blocks'])\n",
    "load_model(model1, \"degradation_transformer_model.safetensors\")\n",
    "model1.eval()\n",
    "learner1 = Learner(model1, optim=None, loss_func=None, \n",
    "                  train_loader=None, test_loader=None, cbs=[])\n",
    "\n",
    "y_predict = learner1.predict(x_batch, num_periods=1)[:, -1]\n",
    "\n",
    "\n",
    "df=pd.DataFrame([y_batch, y_predict]).T\n",
    "df.columns = ['True Value', 'Predicted Value']\n",
    "fig = px.line(df, labels={ 'x':['Predicted Value','Predicted Value']}, title='Test data - Next Token Prediction Accuracy')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f34313-d076-4925-9a02-e13b902cb3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "learner1.predict(np.arange(30), num_periods=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2dc5a9-674e-41db-a7ab-4f60c3450ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "apikey = os.environ.get(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e6923c-86ff-4e31-bc79-be320c10a98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "apikey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda394d9-b95f-4d8e-b8e8-da7cb14cee14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## test model load from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570b0569-81a9-449e-b29d-dc3348738d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact('smasadzadeh-freelancer/degradation-transformer/degradation-transformer-model:Production')\n",
    "\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "safetensor_filename=\"degradation_transformer_model.safetensors\"\n",
    "config_filename=\"degradation_transformer_model_config.json\"\n",
    "\n",
    "model_weights_path = os.path.join(artifact_dir, safetensor_filename)\n",
    "config_path = os.path.join(artifact_dir, config_filename)\n",
    "\n",
    "# Load config and create model\n",
    "model_params = json.load(open(config_path))\n",
    "model3 = DegradationTransformer(**model_params)\n",
    "load_model(model, model_weights_path)\n",
    "\n",
    "learner3 = Learner(model3, optim=None, loss_func=None, \n",
    "                  train_loader=None, test_loader=None, cbs=[])\n",
    "\n",
    "y_predict = learner3.predict(x_batch, num_periods=1)[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "df=pd.DataFrame([y_batch, y_predict]).T\n",
    "df.columns = ['True Value', 'Predicted Value']\n",
    "fig = px.line(df, labels={ 'x':['Predicted Value','Predicted Value']}, title='Test data - Next Token Prediction Accuracy')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e690724-9858-4185-86c8-5a27bbb66766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# test model loading from hf\n",
    "\n",
    "this could be a differnet model than what we train here before pushing code to github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e451b55e-d2cc-4990-8b9b-b39504643a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"smasadzadeh/degradation-transformer\",\n",
    "    filename=\"degradation_transformer_model.safetensors\"\n",
    ")\n",
    "\n",
    "config_file_path = hf_hub_download(\n",
    "    repo_id=\"smasadzadeh/degradation-transformer\",\n",
    "    filename=\"degradation_transformer_model_config.json\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model_params = json.load(open(config_file_path, \"rb\"))\n",
    "# import DegradationTransformer class from utils_file_path\n",
    "\n",
    "model2 = DegradationTransformer(vocab_size=model_params['vocab_size'], \n",
    "                                context_window=model_params['context_window'], \n",
    "                               embedding_dim=model_params['embedding_dim'], \n",
    "                               num_heads=model_params['num_heads'],\n",
    "                                 num_blocks=model_params['num_blocks'])\n",
    "load_model(model2, file_path)\n",
    "\n",
    "\n",
    "learner2 = Learner(model2, optim=None, loss_func=None, \n",
    "                  train_loader=None, test_loader=None, cbs=[])\n",
    "\n",
    "y_predict = learner2.predict(x_batch, num_periods=1)[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "df=pd.DataFrame([y_batch, y_predict]).T\n",
    "df.columns = ['True Value', 'Predicted Value']\n",
    "fig = px.line(df, labels={ 'x':['Predicted Value','Predicted Value']}, title='Test data - Next Token Prediction Accuracy')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c49982-8b7e-4817-b3e9-65cd01fad167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/smasadzade@gmail.com/degradation-transformer/requirements-app.txt"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "smartchp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
